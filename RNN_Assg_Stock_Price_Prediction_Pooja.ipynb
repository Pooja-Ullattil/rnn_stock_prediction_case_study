{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cjf0S2yqD0uf"
   },
   "source": [
    "# Stock Price Prediction Using RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mhNGhRQWjN94"
   },
   "source": [
    "## Objective\n",
    "The objective of this assignment is to try and predict the stock prices using historical data from four companies IBM (IBM), Google (GOOGL), Amazon (AMZN), and Microsoft (MSFT).\n",
    "\n",
    "We use four different companies because they belong to the same sector: Technology. Using data from all four companies may improve the performance of the model. This way, we can capture the broader market sentiment.\n",
    "\n",
    "The problem statement for this assignment can be summarised as follows:\n",
    "\n",
    "> Given the stock prices of Amazon, Google, IBM, and Microsoft for a set number of days, predict the stock price of these companies after that window."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w4gU_Gs--yjG"
   },
   "source": [
    "## Business Value\n",
    "\n",
    "Data related to stock markets lends itself well to modeling using RNNs due to its sequential nature. We can keep track of opening prices, closing prices, highest prices, and so on for a long period of time as these values are generated every working day. The patterns observed in this data can then be used to predict the future direction in which stock prices are expected to move. Analyzing this data can be interesting in itself, but it also has a financial incentive as accurate predictions can lead to massive profits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TC424ieDIlaK"
   },
   "source": [
    "### **Data Description**\n",
    "\n",
    "You have been provided with four CSV files corresponding to four stocks: AMZN, GOOGL, IBM, and MSFT. The files contain historical data that were gathered from the websites of the stock markets where these companies are listed: NYSE and NASDAQ. The columns in all four files are identical. Let's take a look at them:\n",
    "\n",
    "- `Date`: The values in this column specify the date on which the values were recorded. In all four files, the dates range from Jaunary 1, 2006 to January 1, 2018.\n",
    "\n",
    "- `Open`: The values in this column specify the stock price on a given date when the stock market opens.\n",
    "\n",
    "- `High`: The values in this column specify the highest stock price achieved by a stock on a given date.\n",
    "\n",
    "- `Low`: The values in this column specify the lowest stock price achieved by a stock on a given date.\n",
    "\n",
    "- `Close`: The values in this column specify the stock price on a given date when the stock market closes.\n",
    "\n",
    "- `Volume`: The values in this column specify the total number of shares traded on a given date.\n",
    "\n",
    "- `Name`: This column gives the official name of the stock as used in the stock market.\n",
    "\n",
    "There are 3019 records in each data set. The file names are of the format `\\<company_name>_stock_data.csv`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XzV7hepM8K-H"
   },
   "source": [
    "## **1 Data Loading and Preparation** <font color =red> [25 marks] </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q6UQ2zyxH1Ep"
   },
   "source": [
    "#### **Import Necessary Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RyZsIlDgfO3s"
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "!pip install keras-tuner\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.ticker import EngFormatter\n",
    "import pandas as pd\n",
    "import os\n",
    "import tensorflow # Added import for tensorflow\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import Sequential, Input, Model\n",
    "from tensorflow.keras.layers import Embedding, Dense, TimeDistributed, LSTM, GRU, Bidirectional, SimpleRNN, RNN, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from kerastuner.tuners import Hyperband\n",
    "from kerastuner import HyperParameters as hp\n",
    "from keras import optimizers\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H4_Xnhwsl-00"
   },
   "source": [
    "### **1.1 Data Aggregation** <font color =red> [7 marks] </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5clrTkdAvq8G"
   },
   "source": [
    "As we are using the stock data for four different companies, we need to create a new DataFrame that contains the combined data from all four data frames. We will create a function that takes in a list of the file names for the four CSV files, and returns a single data frame. This function performs the following tasks:\n",
    "- Extract stock names from file names\n",
    "- Read the CSV files as data frames\n",
    "- Append the stock names into the columns of their respective data frames\n",
    "- Drop unnecessary columns\n",
    "- Join the data frames into one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3tPMCwNwe1JS"
   },
   "source": [
    "#### **1.1.1** <font color =red> [5 marks] </font>\n",
    "Create the function to join DataFrames and use it to combine the four datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-gj0K2Q65yix"
   },
   "outputs": [],
   "source": [
    "# Define a function to load data and aggregate them\n",
    "def load_data(directory_path, file_names):\n",
    "  aggregated_df = pd.DataFrame()\n",
    "  for file in file_names:\n",
    "    df = pd.read_csv(os.path.join(directory_path,file))\n",
    "    df['Name'] = file.split('_')[0]\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    aggregated_df = pd.concat([aggregated_df,df],axis=0,ignore_index=True)\n",
    "  return aggregated_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r0sYevlt7kgn"
   },
   "outputs": [],
   "source": [
    "# Specify the names of the raw data files to be read and use the aggregation function to read the files\n",
    "directory_path = \"C:\\Users\\USER\\Downloads\\New folder\"\n",
    "file_names = [f for f in os.listdir(directory_path) if f.endswith('.csv')]\n",
    "master_df = load_data(directory_path, file_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4qMKdhGj8cKF"
   },
   "outputs": [],
   "source": [
    "# View specifics of the data\n",
    "print('Shape of master df: ',master_df.shape)\n",
    "master_df.tail()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lwVzhIuBfRGn"
   },
   "source": [
    "#### **1.1.2** <font color =red> [2 marks] </font>\n",
    "Identify and handle any missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7vmQGhf69x36"
   },
   "outputs": [],
   "source": [
    "# Handle Missing Values\n",
    "print(master_df.info())\n",
    "master_df = master_df.dropna()\n",
    "print(master_df.info())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YuFFxr5jQ4xw"
   },
   "source": [
    "### **1.2 Analysis and Visualisation** <font color =red> [5 marks] </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YnybVo_YQ4xx"
   },
   "source": [
    "#### **1.2.1** <font color =red> [2 marks] </font>\n",
    "Analyse the frequency distribution of stock volumes of the companies and also see how the volumes vary over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fGxyKbfRcFl-"
   },
   "outputs": [],
   "source": [
    "# Frequency distribution of volumes\n",
    "def plot_histograms(df, stock_name):\n",
    "  sns.histplot(data=df[df['Name']==stock_name], x='Volume', bins=25, kde=True)\n",
    "  plt.title('Frequency Distribution of Trading Volume for '+stock_name)\n",
    "  plt.xlabel('Vol')\n",
    "  plt.ylabel('Freq')\n",
    "  plt.gca().xaxis.set_major_formatter(EngFormatter())\n",
    "  plt.show()\n",
    "\n",
    "for stock_name in master_df['Name'].unique():\n",
    "  plot_histograms(master_df, stock_name)\n",
    "  print('\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3Q1g9Y0JcNW0"
   },
   "outputs": [],
   "source": [
    "# Stock volume variation over time\n",
    "def plot_lines_volumes(df, stock_name):\n",
    "  sns.lineplot(data=df[df['Name']==stock_name], x='Date', y='Vol', hue='Name')\n",
    "  plt.title('Variation of Trading Volume over Time for '+stock_name)\n",
    "  plt.xlabel('Date')\n",
    "  plt.ylabel('Vol')\n",
    "  plt.gca().yaxis.set_major_formatter(EngFormatter())\n",
    "  plt.show()\n",
    "for stock_name in master_df['Name'].unique():\n",
    "  plot_lines_volumes(master_df, stock_name)\n",
    "  print('\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QO5frnjURc5i"
   },
   "source": [
    "#### **1.2.2** <font color =red> [3 marks] </font>\n",
    "Analyse correlations between features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SE9ijOnmP94M"
   },
   "outputs": [],
   "source": [
    "# Analyse correlations\n",
    "def plot_correlation_matrix(df, stock_name):\n",
    "  corr = df[df['Name']==stock_name].corr(numeric_only=True)\n",
    "  sns.heatmap(corr, annot=True, cmap='coolwarm')\n",
    "  plt.title('Correlation Matrix for '+stock_name)\n",
    "  plt.show()\n",
    "\n",
    "for stock_name in master_df['Name'].unique():\n",
    "  plot_correlation_matrix(master_df, stock_name)\n",
    "  print('\\n')\n",
    "\n",
    "corr = master_df.corr(numeric_only=True)\n",
    "sns.heatmap(corr, annot=True, cmap='coolwarm')\n",
    "plt.title('Overall Correlation Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t1dHdCDQl-1K"
   },
   "source": [
    "### **1.3 Data Processing** <font color =red> [13 marks] </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZjjEYTmOopW4"
   },
   "source": [
    "Next, we need to process the data so that it is ready to be used in recurrent neural networks. You know RNNs are suitable to work with sequential data where patterns repeat at regular intervals.\n",
    "\n",
    "For this, we need to execute the following steps:\n",
    "1. Create windows from the master data frame and obtain windowed `X` and corresponding windowed `y` values\n",
    "2. Perform train-test split on the windowed data\n",
    "3. Scale the data sets in an appropriate manner\n",
    "\n",
    "We will define functions for the above steps that finally return training and testing data sets that are ready to be used in recurrent neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aTLu14Wuid5n"
   },
   "source": [
    "**Hint:** If we use a window of size 3, in the first window, the rows `[0, 1, 2]` will be present and will be used to predict the value of `CloseAMZN` in row `3`. In the second window, rows `[1, 2, 3]` will be used to predict `CloseAMZN` in row `4`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mulMxtd6fsFR"
   },
   "source": [
    "#### **1.3.1** <font color =red> [3 marks] </font>\n",
    "Create a function that returns the windowed `X` and `y` data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hXQOh3iP6W7L"
   },
   "source": [
    "From the main DataFrame, this function will create windowed DataFrames, and store those as a list of DataFrames.\n",
    "\n",
    "Controllable parameters will be window size, step size (window stride length) and target names as a list of the names of stocks whose closing values we wish to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-n1M1-4fvq8i"
   },
   "outputs": [],
   "source": [
    "# Define a function that divides the data into windows and generates target variable values for each window\n",
    "def windows(master_df, window_size, step_size, target_names):\n",
    "  X  = []              # X\n",
    "  y = []              # y\n",
    "  df = master_df.copy()\n",
    "  df = df.sort_values(by=['Date']) \n",
    "  \n",
    "  if 'Name' in df.columns:\n",
    "    df = pd.get_dummies(df, columns=['Name'], dtype=int)\n",
    "\n",
    "  \n",
    "  feature_cols = [col for col in df.columns if col != 'Date']\n",
    "\n",
    "  for i in range(0, df.shape[0] - window_size -1 , step_size): \n",
    "    window_features = df[feature_cols].iloc[i: i + window_size]\n",
    "\n",
    "  \n",
    "    target_rows = df[(df['Date'] == df['Date'].iloc[i + window_size]) & (df[target_names_encoded].any(axis=1))] \n",
    "\n",
    "    if not target_rows.empty:\n",
    "        target_values = target_rows['Close'].values[0] \n",
    "        X.append(window_features.values)\n",
    "        y.append(target_values)\n",
    "\n",
    "\n",
    "  return np.array(X), np.array(y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gw5JL8tgmwc-"
   },
   "source": [
    "#### **1.3.2** <font color =red> [3 marks] </font>\n",
    "Create a function to scale the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IBaJkaIx_1Vg"
   },
   "source": [
    "Define a function that will scale the data.\n",
    "\n",
    "For scaling, we have to look at the whole length of data to find max/min values or standard deviations and means. If we scale the whole data at once, this will lead to data leakage in the windows. This is not necessarily a problem if the model is trained on the complete data with cross-validation.\n",
    "\n",
    "One way to scale when dealing with windowed data is to use the `partial_fit()` method.\n",
    "```\n",
    "scaler.partial_fit(window)\n",
    "scaler.transform(window)\n",
    "```\n",
    "You may use any other suitable way to scale the data properly. Arrive at a reasonable way to scale your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n9586NZptrAi"
   },
   "outputs": [],
   "source": [
    "# Define a function that scales the windowed data\n",
    "# The function takes in the windowed data sets and returns the scaled windows\n",
    "def normalize_windows(windows_data, scalers, feature_cols, fit=False):\n",
    "    normalized = []\n",
    "    for window in windows_data:\n",
    "        normalized_window = window.copy()\n",
    "        for i, col in enumerate(feature_cols):\n",
    "            if col in scalers: # Only scale features that have a scaler\n",
    "                if fit:\n",
    "                    scalers[col].partial_fit(normalized_window[:, i].reshape(-1, 1))\n",
    "                normalized_window[:, i] = scalers[col].transform(normalized_window[:, i].reshape(-1, 1)).flatten()\n",
    "        normalized.append(normalized_window)\n",
    "    return np.array(normalized)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C3HhUS59DaCQ"
   },
   "source": [
    "Next, define the main function that will call the windowing and scaling helper functions.\n",
    "\n",
    "The input parameters for this function are:\n",
    "- The joined master data set\n",
    "- The names of the stocks that we wish to predict the *Close* prices for\n",
    "- The window size\n",
    "- The window stride\n",
    "- The train-test split ratio\n",
    "\n",
    "The outputs from this function are the scaled dataframes:\n",
    "- *X_train*\n",
    "- *y_train*\n",
    "- *X_test*\n",
    "- *y_test*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tND37dF1ksmy"
   },
   "source": [
    "#### **1.3.3** <font color =red> [3 marks] </font>\n",
    "Define a function to create windows of `window_size` and split the windowed data in to training and validation sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p3acwSBoArWU"
   },
   "source": [
    "The function can take arguments such as list of target names, window size, window stride and split ratio. Use the windowing function here to make windows in the data and then perform scaling and train-test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dvWRH5J4vq8l"
   },
   "outputs": [],
   "source": [
    "# Define a function to create input and output data points from the master DataFrame\n",
    "feature_scalers = {} \n",
    "y_scaler = StandardScaler() \n",
    "def create_data(master_df, target_names, window_size, window_stride, test_size, validation_size):\n",
    "  df_temp_encoded = pd.get_dummies(master_df.copy(), columns=['Name'], dtype=int)\n",
    "  global target_names_encoded \n",
    "  target_names_encoded = [col for col in df_temp_encoded.columns if col.startswith('Name_') and col.split('_')[1] in target_names]\n",
    "\n",
    "\n",
    "  X, y = windows(master_df, window_size, window_stride, target_names)\n",
    "\n",
    "  \n",
    "  feature_cols = [col for col in df_temp_encoded.columns if col not in ['Date'] + target_names_encoded] # Exclude Date and target encoded names\n",
    "\n",
    "  \n",
    "  if not feature_scalers:\n",
    "      for col in feature_cols:\n",
    "          feature_scalers[col] = StandardScaler()\n",
    "\n",
    " \n",
    "  X_scaled = normalize_windows(X, feature_scalers, feature_cols, fit=True)\n",
    "\n",
    "  \n",
    "  y_scaler = StandardScaler()\n",
    "  y_scaled = y_scaler.fit_transform(y.reshape(-1, 1)).flatten() # Scale y separately\n",
    "\n",
    "  \n",
    "  X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_scaled, test_size=test_size, shuffle=False)\n",
    "\n",
    "  \n",
    "  X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=validation_size/(1-test_size), shuffle=False) # Adjust validation size based on remaining data\n",
    "\n",
    "\n",
    "  return X_train, X_val, X_test, y_train, y_val, y_test, y_scaler \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H5PZw7SHGUdx"
   },
   "source": [
    "We can now use these helper functions to create our training and testing data sets. But first we need to decide on a length of windows. As we are doing time series prediction, we want to pick a sequence that shows some repetition of patterns.\n",
    "\n",
    "For selecting a good sequence length, some business understanding will help us. In financial scenarios, we can either work with business days, weeks (which comprise of 5 working days), months, or quarters (comprising of 13 business weeks). Try looking for some patterns for these periods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mihb_duwxWeC"
   },
   "source": [
    "#### **1.3.4** <font color =red> [2 marks] </font>\n",
    "Identify an appropriate window size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UG-LrkMVjgT-"
   },
   "source": [
    "For this, you can use plots to see how target variable is varying with time. Try dividing it into parts by weeks/months/quarters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nhhbomJIrlK_"
   },
   "outputs": [],
   "source": [
    "# Checking for patterns in different sequence lengths\n",
    "def plot_lines_Close(df, stock_name,time_period='dai'):\n",
    "  df = master_df[master_df['Name']==stock_name].copy()\n",
    "  df = df.set_index('Date')\n",
    "  if time_period == 'week':\n",
    "    df = df.resample('W').max()\n",
    "  elif time_period == 'month':\n",
    "    df = df.resample('M').max()\n",
    "  elif time_period == 'quarter':\n",
    "    df = df.resample('Q').max()\n",
    "  else:\n",
    "    df = df\n",
    "  sns.lineplot(data=df, x='Date', y='Close', hue='Name')\n",
    "  plt.title('Variation of Closing Price over '+ time_period +'ly Timeframe for '+stock_name)\n",
    "  plt.xlabel('Date')\n",
    "  plt.ylabel('Close')\n",
    "  plt.gca().yaxis.set_major_formatter(EngFormatter())\n",
    "  plt.show()\n",
    "\n",
    "for stock_name in master_df['Name'].unique():\n",
    "  plot_lines_Close(master_df, stock_name)\n",
    "  plot_lines_Close(master_df, stock_name,time_period='week')\n",
    "  plot_lines_Close(master_df, stock_name,time_period='month')\n",
    "  plot_lines_Close(master_df, stock_name,time_period='quarter')\n",
    "  print('\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KZpKaENfyblo"
   },
   "source": [
    "#### **1.3.5** <font color =red> [2 marks] </font>\n",
    "Call the functions to create testing and training instances of predictor and target features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-sFdDgH0vq8o"
   },
   "outputs": [],
   "source": [
    "# Create data instances from the master data frame using decided window size and window stride\n",
    "X_train, X_val, X_test, y_train, y_val, y_test, y_scaler = create_data(master_df.copy(), ['AMZN'], 65, 5, 0.2, 0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JoRLGqp-6aHP"
   },
   "outputs": [],
   "source": [
    "# Check the number of data points generated\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "51IV5zmjBf-w"
   },
   "source": [
    "**Check if the training and testing datasets are in the proper format to feed into neural networks.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ViVKgkrwvq8r"
   },
   "outputs": [],
   "source": [
    "# Check if the datasets are compatible inputs to neural networks\n",
    "print('X_train:',X_train[0])\n",
    "print('\\n\\n')\n",
    "print('y_train:',y_train[0])\n",
    "print('\\n\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fNNSAT6YfO3z"
   },
   "source": [
    "## **2 RNN Models** <font color =red> [20 marks] </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vqHs0PjzwCve"
   },
   "source": [
    "In this section, we will:\n",
    "- Define a function that creates a simple RNN\n",
    "- Tune the RNN for different hyperparameter values\n",
    "- View the performance of the optimal model on the test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ekq-LY1p86NI"
   },
   "source": [
    "### **2.1 Simple RNN Model** <font color =red> [10 marks] </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tdzl5ojcyDX7"
   },
   "source": [
    "#### **2.1.1** <font color =red> [3 marks] </font>\n",
    "Create a function that builds a simple RNN model based on the layer configuration provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "owGKpgMxRtk2"
   },
   "outputs": [],
   "source": [
    "# Create a function that creates a simple RNN model according to the model configuration arguments\n",
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "\n",
    "    # Tune layers and units\n",
    "    for i in range(hp.Int('num_layers', 1, 3)):\n",
    "        model.add(SimpleRNN(\n",
    "            units=hp.Int(f'units_{i}', 32, 128, step=32),\n",
    "            activation=hp.Choice('activation', ['tanh', 'relu']),\n",
    "            return_sequences=(i < hp.Int('num_layers', 1, 3)-1)  # Only last layer returns sequence=False\n",
    "        ))\n",
    "\n",
    "    model.add(Dense(1))  # Output layer for regression\n",
    "\n",
    "    # Tune optimizer and learning rate\n",
    "    optimizer = hp.Choice('optimizer', ['adam', 'rmsprop'])\n",
    "    lr = hp.Float('lr', 1e-4, 1e-2, sampling='log')\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizers.get({'class_name': optimizer, 'config': {'learning_rate': lr}}),\n",
    "        loss='mse',\n",
    "        metrics=['mae']\n",
    "    )\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R4xDFvOXfO31"
   },
   "source": [
    "#### **2.1.2** <font color =red> [4 marks] </font>\n",
    "Perform hyperparameter tuning to find the optimal network configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jGRsV3GefO31",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Find an optimal configuration of simple RNN\n",
    "tuner = Hyperband(\n",
    "    build_model,\n",
    "    objective='val_loss',\n",
    "    max_epochs=50,\n",
    "    factor=3,\n",
    "    directory='tuner_dir',\n",
    "    project_name='rnn_tuning'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eHuJ61iYfO31"
   },
   "outputs": [],
   "source": [
    "# Find the best configuration based on evaluation metrics\n",
    "tuner.search(\n",
    "    X_train, y_train,\n",
    "    epochs=50,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=[\n",
    "        tensorflow.keras.callbacks.EarlyStopping(patience=5)\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "best_model = tuner.get_best_models(num_models=1)[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8krOJTq5_fM0"
   },
   "source": [
    "#### **2.1.3** <font color =red> [3 marks] </font>\n",
    "Run for optimal Simple RNN Model and show final results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qLQPoIQCfO32",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create an RNN model with a combination of potentially optimal hyperparameter values and retrain the model\n",
    "best_hp = tuner.get_best_hyperparameters(num_trials=1)[0] # This was already done in a previous step\n",
    "\n",
    "best_simple_rnn_model = best_model\n",
    "history_simple_rnn = best_simple_rnn_model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=100, # You can increase epochs if needed, early stopping will prevent overfitting\n",
    "    validation_data=(X_val, y_val)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-QceWx7D78RH"
   },
   "source": [
    "Plotting the actual vs predicted values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yK1yY499Ynpq"
   },
   "outputs": [],
   "source": [
    "# Predict on the test data and plot\n",
    "y_pred_simple_rnn = best_simple_rnn_model.predict(X_test)\n",
    "\n",
    "# Inverse transform the scaled predictions and actual values to their original scale\n",
    "y_pred_simple_rnn_original = y_scaler.inverse_transform(y_pred_simple_rnn)\n",
    "y_test_original = y_scaler.inverse_transform(y_test.reshape(-1, 1))\n",
    "\n",
    "# Plotting the actual vs predicted values\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(y_test_original, label='Actual')\n",
    "plt.plot(y_pred_simple_rnn_original, label='Predicted')\n",
    "plt.title('Simple RNN: Actual vs Predicted Stock Prices')\n",
    "plt.xlabel('Time Steps')\n",
    "plt.ylabel('Stock Price')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1GUfu3167Rqx"
   },
   "source": [
    "It is worth noting that every training session for a neural network is unique. So, the results may vary slightly each time you retrain the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aHlPLvDcfO32"
   },
   "outputs": [],
   "source": [
    "# Compute the performance of the model on the testing data set\n",
    "loss, mae = best_simple_rnn_model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "print(f'Test Loss: {loss:.4f}')\n",
    "print(f'Test MAE: {mae:.4f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lLmMGXvyvq9V"
   },
   "source": [
    "### **2.2 Advanced RNN Models** <font color =red> [10 marks] </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nCaJYIHzwzP9"
   },
   "source": [
    "In this section, we will:\n",
    "- Create an LSTM or a GRU network\n",
    "- Tune the network for different hyperparameter values\n",
    "- View the performance of the optimal model on the test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O6KBxY4Jasm5"
   },
   "source": [
    "#### **2.2.1** <font color =red> [3 marks] </font>\n",
    "Create a function that builds an advanced RNN model with tunable hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0g1yDzllvq9W"
   },
   "outputs": [],
   "source": [
    "# # Define a function to create a model and specify default values for hyperparameters\n",
    "def build_gru_model(hp):\n",
    "    model = Sequential()\n",
    "\n",
    "\n",
    "    for i in range(hp.Int('num_layers', 1, 3)):\n",
    "        model.add(GRU(\n",
    "            units=hp.Int(f'units_{i}', 32, 256, step=32),\n",
    "            return_sequences=(i < hp.Int('num_layers', 1, 3) - 1),\n",
    "            activation=hp.Choice('gru_activation', ['tanh', 'relu'])\n",
    "        ))\n",
    "        \n",
    "        if hp.Boolean(f'dropout_{i}'):\n",
    "            model.add(Dropout(\n",
    "                rate=hp.Float(f'dropout_rate_{i}', 0.1, 0.5, step=0.1)\n",
    "            ))\n",
    "\n",
    "    \n",
    "    model.add(Dense(1, activation='linear'))\n",
    "\n",
    "    \n",
    "    lr = hp.Float('lr', 1e-4, 1e-2, sampling='log')\n",
    "    model.compile(\n",
    "        optimizer=optimizers.Adam(learning_rate=lr),\n",
    "        loss='mse',\n",
    "        metrics=['mae']\n",
    "    )\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G0lvqIJ2vq9b"
   },
   "source": [
    "#### **2.2.2** <font color =red> [4 marks] </font>\n",
    "Perform hyperparameter tuning to find the optimal network configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sWRBShecvq9e",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Find an optimal configuration\n",
    "tensorflow.config.optimizer.set_jit(True)  # Enable XLA\n",
    "\n",
    "tuner = Hyperband(\n",
    "    build_gru_model,\n",
    "    objective='val_loss',\n",
    "    max_epochs=50,\n",
    "    factor=3,\n",
    "    hyperband_iterations=1,\n",
    "    directory='gru_tuning',\n",
    "    project_name='stock_forecasting',\n",
    "    overwrite=True  # Overwrite previous runs\n",
    ")\n",
    "\n",
    "early_stop = tensorflow.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', patience=5, restore_best_weights=True\n",
    ")\n",
    "\n",
    "tuner.search(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=50,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "best_model = tuner.hypermodel.build(best_hps)\n",
    "\n",
    "print(f\"Best GRU units: {best_hps.get('units_0')}\")\n",
    "print(f\"Best layers: {best_hps.get('num_layers')}\")\n",
    "print(f\"Best learning rate: {best_hps.get('lr')}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_n5OTBdSvq9t"
   },
   "source": [
    "#### **2.2.3** <font color =red> [3 marks] </font>\n",
    "Run for optimal RNN Model and show final results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5uyN6vgzvq9w",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create the model with a combination of potentially optimal hyperparameter values and retrain the model\n",
    "history_gru = best_model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=100,  # You can adjust the number of epochs\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=[early_stop] # Use the early stopping callback\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1x9njgzwvq92"
   },
   "outputs": [],
   "source": [
    "# Compute the performance of the model on the testing data set\n",
    "y_pred_simple_rnn = best_model.predict(X_test)\n",
    "\n",
    "y_pred_simple_rnn_original = y_scaler.inverse_transform(y_pred_simple_rnn)\n",
    "y_test_original = y_scaler.inverse_transform(y_test.reshape(-1, 1))\n",
    "\n",
    "\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(y_test_original, label='Actual')\n",
    "plt.plot(y_pred_simple_rnn_original, label='Predicted')\n",
    "plt.title('Simple RNN: Actual vs Predicted Stock Prices')\n",
    "plt.xlabel('Time Steps')\n",
    "plt.ylabel('Stock Price')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "36Hy584ffhsS"
   },
   "source": [
    "Plotting the actual vs predicted values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "31CshI-SfhsS"
   },
   "outputs": [],
   "source": [
    "# Predict on the test data\n",
    "loss, mae = best_model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "print(f'Test Loss: {loss:.4f}')\n",
    "print(f'Test MAE: {mae:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EStsPXCYf3_q"
   },
   "source": [
    "## **3 Predicting Multiple Target Variables** <font color =red> [OPTIONAL] </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JKaiTII-23Aq"
   },
   "source": [
    "In this section, we will use recurrent neural networks to predict stock prices for more than one company."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "taBw30-HLTWV"
   },
   "source": [
    "### **3.1 Data Preparation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A9aHdR6EgVqT"
   },
   "source": [
    "#### **3.1.1**\n",
    "Create testing and training instances for multiple target features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ua3P_ySxgq9Z"
   },
   "source": [
    "You can take the closing price of all four companies to predict here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BQEWm-m129Rw"
   },
   "outputs": [],
   "source": [
    "# Create data instances from the master data frame using a window size of 65, a window stride of 5 and a test size of 20%\n",
    "# Specify the list of stock names whose 'Close' values you wish to predict using the 'target_names' parameter\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4_WWJKvsLlAA"
   },
   "outputs": [],
   "source": [
    "# Check the number of data points generated\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4kUyl2U9LnpK"
   },
   "source": [
    "### **3.2 Run RNN Models**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c2HTWIcWhLE0"
   },
   "source": [
    "#### **3.2.1**\n",
    "Perform hyperparameter tuning to find the optimal network configuration for Simple RNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FhjXH72pMI4t",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Find an optimal configuration of simple RNN\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DYXngNUmMI4u"
   },
   "outputs": [],
   "source": [
    "# Find the best configuration\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-vyWykGBMI4y",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create an RNN model with a combination of potentially optimal hyperparameter values and retrain the\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4j63wDQkMcNB"
   },
   "outputs": [],
   "source": [
    "# Compute the performance of the model on the testing data set\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4QbM-hR6h-SV"
   },
   "outputs": [],
   "source": [
    "# Plotting the actual vs predicted values for all targets\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vnwOVGv5MjXi"
   },
   "source": [
    "#### **3.2.2**\n",
    "Perform hyperparameter tuning to find the optimal network configuration for Advanced RNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vQRqZZbEIrh9",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Find an optimal configuration of advanced RNN\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PZswZ55JIrh-"
   },
   "outputs": [],
   "source": [
    "# Find the best configuration\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3Fh-2tBTNWXI",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a model with a combination of potentially optimal hyperparameter values and retrain the model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_y7m-nziNWXK"
   },
   "outputs": [],
   "source": [
    "# Compute the performance of the model on the testing data set\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3OMsd4hHicRO"
   },
   "outputs": [],
   "source": [
    "# Plotting the actual vs predicted values for all targets\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YItIF9_SmeCN"
   },
   "source": [
    "## **4 Conclusion** <font color =red> [5 marks] </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qGwEyQn1meCN"
   },
   "source": [
    "### **4.1 Conclusion and insights** <font color =red> [5 marks] </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RqBneTdDaVYw"
   },
   "source": [
    "#### **4.1.1** <font color =red> [5 marks] </font>\n",
    "Conclude with the insights drawn and final outcomes and results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This case study contributes to a deeper knowledge of the operation of RNN models. \n",
    "#The data was first loaded using a function, and each CSV file was stacked vertically while being distinguished by the Name column.\n",
    "#The stock's unique name appears in the Name column.  \n",
    "#The volume and frequency of each stock were then examined over time.  \n",
    "#The volume frequency varies among stocks, and there isn't much of a pattern to follow.\n",
    "#Next, we looked at relationships between the various data features.  \n",
    "#\n",
    "#There is a strong correlation between Open, High, Low, and Close. \n",
    "#To examine the close price over a period of days, weeks, months, or quarters, we have created line plots. \n",
    "#Later, we created 65-day windows using five processes.  built and standardized those windows.\n",
    "#Later, we created 65-day windows using five processes. \n",
    "#built and standardized those windows. Next, we divided the data into sets for testing, validation, and training. \n",
    "#Simple RNN and GRU models are constructed using defined functions, and they are hypertuned using Hyperband.\n",
    "#Simple RNN Model:¶\n",
    "#Test Loss: 2.407\n",
    "#Test MAE: 1.441\n",
    "#GRU Model:\n",
    "#Test Loss: 2.030\n",
    "#Test MAE: 1.316"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1YH9zvGTQCZrVKcWwV8QeTmIyR6cGfcX4",
     "timestamp": 1740056154426
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
